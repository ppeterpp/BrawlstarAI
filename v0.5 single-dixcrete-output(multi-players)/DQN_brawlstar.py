import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import random
import time
from tqdm import tqdm
import game_wrapped
from networks_transformer import MultiModalEncoder
from multi_thread import *
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
import matplotlib
import matplotlib.pyplot as plt

'''
each return observation includes dimensions as follows:
x = [torch.zeros((time_step,128,40,80)),
     torch.zeros((time_step,256,20,40)),
     torch.zeros((time_step,512,10,20)),
     torch.zeros((time.step,10))]
'''

# multi-model-encoder includes conv encoder for feature map generated by yolo-v5s,
# as well as fully connected encoder for a game-entity-info vector
c_out = 8  # convolution output channel
l_out = 128  # linear encoder output dimension
input_size = c_out*(40*80 + 20*40 + 10*20) + l_out  # lstm input-size
action_dim_l = 9  # left hand options
action_dim_r = 3  # right hand options
action_dim_mix = action_dim_l*action_dim_r
time_step = 2  # the net takes 'time_step' frames of data each time
device = 'cuda'
render = False

camera = CameraThread()
camera_assist = CameraAssistThread(camera)
yolothread = YoloThread(camera, camera_assist, name='yolothread')
env = game_wrapped.BrawlEnv(watcher=yolothread, time_step=time_step, render=render)
env = env.unwrapped  # gym对游戏进行了封装，有些变量变得不可访问，这里使游戏内部的变量可以访问

# 两个网络，进行延迟更新，即不能每次动作都更新网络，
# 要隔一段时间进行一次更新，否则奖励累积可能爆炸
net = MultiModalEncoder(c_out, l_out, input_size, action_dim_mix, device).to(device)
net2 = MultiModalEncoder(c_out, l_out, input_size, action_dim_mix, device).to(device)

store_count = 0  # 经验池，将智能体碰到的各种情况都记录下来
store_size = 256  # buffer size，即经验池能存多少条数据
decline = 0.9  # 衰减系数
learn_time = 0  # 学习次数
update_time = 20  # 隔多少个学习次数更新一次网络
gama = 0.9  # Q值计算时的折扣因子
b_size = 128  # batch size，一次从经验池中抽取多少数据进行学习
start_study = False
loss = 0
loss_history = []
# score_history = []
iter_history = []

if os.path.exists('history/model_params.pkl'):  # and False:
    print('---------loading trained model---------')
    net.load_state_dict(torch.load('history/model_params.pkl'))
    net2.load_state_dict(torch.load('history/model_params.pkl'))
    learn_time = int(open('history/learn_time.txt', 'r').read())
    load = torch.load('history/loss_history.pth')
    iter_history = load['iter_history'].tolist()
    loss_history = load['loss_history'].tolist()
    print('learned time: ', learn_time)
    print('loss history length: ', len(loss_history))
elif not os.path.exists('history'):
    os.mkdir('history')

# 初始化buffer 列中储存 s, a, s_, r
# 即s状态下进行操作a会跳转至s_，得到reward
# cartpole中state(observation)长度为4，action、reward长度为1
# 因此经验池中每条数据的长度为obsv_dim+1+obsv_dim+1
store_s1 = torch.zeros((store_size, time_step, 128, 40, 80))   # s
store_s2 = torch.zeros((store_size, time_step, 256, 20, 40))
store_s3 = torch.zeros((store_size, time_step, 512, 10, 20))
store_s4 = torch.zeros((store_size, time_step, 6, 6))
store_s5 = torch.zeros((store_size, time_step, 6))
store_s1_ = torch.zeros((store_size, time_step, 128, 40, 80))  # s_
store_s2_ = torch.zeros((store_size, time_step, 256, 20, 40))
store_s3_ = torch.zeros((store_size, time_step, 512, 10, 20))
store_s4_ = torch.zeros((store_size, time_step, 6, 6))
store_s5_ = torch.zeros((store_size, time_step, 6))
store_action = torch.zeros((store_size, 1))                    # a
store_reward = torch.zeros((store_size, 1))                    # r

camera.start()
camera_assist.start()
time.sleep(1)
yolothread.start()
iter = 3000
pbar = tqdm(total=iter)
train = False
reward_history = []
mean_r_history = []
matplotlib.use('TkAgg')
# plt.figure(figsize=(480, 480))
for i in range(iter):
    s = env.reset()  # 初始化游戏env，s即为state (time_step * obsv_dim)
    idx = random.randint(0, 2)
    for i in range(4):
        env.step(env.game_state.action_mix[[2,14,17][idx]])
    for i in range(3):
        env.step(env.game_state.action_mix[[2,14, 17][random.randint(0, 2)]])
    idx = random.randint(0, 2)
    for i in range(4):
        env.step(env.game_state.action_mix[[2,14, 17][idx]])
    start_time = time.time()
    while True:
        # print('store count:', store_count)
        if random.randint(0, 100) < 100 * (decline ** (learn_time * 0.01)):
            # 随着学习次数(learn_time)的增加，乘上decline也越小(decline<1)
            # 即越到后期，操作越不可能是随机生成
            a = random.randint(0, action_dim_mix-1)
        else:
            # 与上面随机生成action相对，此处采用神经网络输出选择action
            # out为[左走累计奖励, 右走累计奖励]
            # 累计奖励即此刻到游戏结束所有奖励之和
            # 每次反向传播之前都有梯度归零，所以后面的detach()可以不用
            # unsqueeze(0)为在最前端加上维度batch=1
            out = net([s[0].unsqueeze(0).to(device),
                       s[1].unsqueeze(0).to(device),
                       s[2].unsqueeze(0).to(device),
                       s[3].unsqueeze(0).to(device),
                       s[4].unsqueeze(0).to(device)]).detach()
            # torch.max返回的是元组(max, index)
            # 此处argmax则只返回最大值的索引，是个tensor，要转化后提取
            a = torch.argmax(out).data.item()  # 索引值012正好对应三种action
        s_, r, done, info = env.step(env.game_state.action_mix[a])  # s_即observation，info没什么用
        reward_history.append(r)
        mean_r_history.append(np.mean(reward_history))
        plt.plot(mean_r_history, color='royalblue')
        # 奖励r的设计最难也最重要
        # reward的范围在0~1，防止累积值过大

        # 此处添加回放池数据，由于长度为2000，第2001条将覆盖第1条数据
        # print(s.shape, a, s_.shape, r)
        # print(type(s), type(a), s_.shape, type(r))
        if train:
            store_s1[store_count % store_size] = s[0]    # s
            store_s2[store_count % store_size] = s[1]
            store_s3[store_count % store_size] = s[2]
            store_s4[store_count % store_size] = s[3]
            store_s5[store_count % store_size] = s[4]
            store_s1_[store_count % store_size] = s_[0]  # s_
            store_s2_[store_count % store_size] = s_[1]
            store_s3_[store_count % store_size] = s_[2]
            store_s4_[store_count % store_size] = s_[3]
            store_s5_[store_count % store_size] = s_[4]
            store_action[store_count % store_size] = a   # a
            store_reward[store_count % store_size] = r   # r
            store_count += 1
        s = s_

        if store_count > store_size and train:  # 判断条件意为回放池已满(可以进行训练了)
            if learn_time % update_time == 0:
                # 每经过一个update_time，更新net2
                # 将net的权重提取、加载到net2
                # net则是实时更新的，如后面
                net2.load_state_dict(net.state_dict())

                if not isinstance(loss, int):
                    loss_history.append(torch.Tensor.cpu(loss).detach())  # detach()将去除所带梯度
                    # score_history.append(env.peak_score)
                    iter_history.append(learn_time)

                if learn_time % (update_time * 20) == 0:
                    if os.path.exists('history/model_params.pkl'):
                        os.remove('history/model_params.pkl')
                        print('---------权重保存数据更新---------')
                    else:
                        # pass
                        print('---------权重数据进行保存---------')
                    torch.save(net2.state_dict(), 'history/model_params.pkl')

                    # 训练时有类似epsilon greedy的效果，即随着learn_time增加，也即模型愈发完善，随机action的几率会降低
                    # 所以在保存模型权重的时候也要保存这个量，在加载时一并加载，否则每次重新运行初期都会大量随机动作
                    # with open('history/learn_time.txt', 'w') as file:
                    #     file.write(str(learn_time))

                    # loss数据保存
                    # save = {'iter_history': torch.Tensor(iter_history), 'loss_history': torch.Tensor(loss_history)}
                    # torch.save(save, 'history/loss_history.pth')

            # 随机从某处开始，连续抽取b_size(1000)条数据用于神经网络的训练
            index = random.randint(0, store_size - b_size - 1)
            b_s = [store_s1[index:index + b_size].to(device),  # each: batch * time-step * feature-map dirs
                   store_s2[index:index + b_size].to(device),
                   store_s3[index:index + b_size].to(device),
                   store_s4[index:index + b_size].to(device),
                   store_s5[index:index + b_size].to(device)]
            b_a = torch.Tensor(store_action[index:index + b_size]).long().to(device)  # b * t * 1
            b_s_ = [store_s1_[index:index + b_size].to(device),  # as b_s
                    store_s2_[index:index + b_size].to(device),
                    store_s3_[index:index + b_size].to(device),
                    store_s4_[index:index + b_size].to(device),
                    store_s5_[index:index + b_size].to(device)]
            b_r = torch.Tensor(store_reward[index:index + b_size]).to(device)  # b * t * 1

            # 抽取出的b_a为前面探索时的action记录，某些是随机选取的，某些是参考网络做出的
            # 此处的b_s有1000条，送入net生成1000对输出，每一对都是对向左向右两种操作的判断(左右各对应的q值)
            # 即[[r1_1,r1_2], [r2_1,r2_2], ...]共1000对，这一整条数据具有"两个维度"
            # gather中的1就是说，对第二个维度进行操作，b_a则是说对第二个维度每一对数据选取哪一个
            # 例如b_a是[0,1...]即第一次向左，第二次向右
            # 则gather将对应提取并整合出[r1_1,r2_2...]
            q = net(b_s).gather(1, b_a)

            # detach()是截断梯度流，反向传播时有用
            # 注意此处计算q_next用的是net2，是有一个滞后效果的，因为net2是隔一段时间才会更新一次
            # net代入b_s_的输出同样为[[r1_1,r1_2], [r2_1,r2_2], ...]，这是对下一个state做不同action对应累积reward的估计
            # 之前的话是依照历史action筛选历史reward，此处是直接根据net做出reward选择，所以此处直接选择每一对中最大的reward
            # max返回的是(max, index)元组，我们要的是值；max(1)的1与上面的gather的1类似，是对第二个维度操作

            # solution 1/2: DQN
            # q_next = net2(b_s_).detach().max(1)[0].reshape(b_size, 1)  # reshape把1*1000转置成1000*1，才能相加

            # solution 2/2: Double DQN
            q_target_next = net2(b_s_).detach()
            q_eval_next = net(b_s_).detach()
            q_next = q_target_next.gather(1, q_eval_next.argmax(axis=1).reshape(-1, 1))

            # Q value计算公式
            tq = b_r + gama * q_next

            loss = F.smooth_l1_loss(q, tq)
            # loss = net.mls(q, tq)
            net.opt.zero_grad()
            loss.backward()
            net.opt.step()
            if i % 60 == 0:
                net.dyna_lr.step()  # 更新学习率

            learn_time += 1
            if not start_study:
                print('start study')
                start_study = True
                break
        plt.pause(0.01)
        plt.ioff()
        if len(reward_history) % 100 == 0:
            reward_save = {'reward': torch.Tensor(reward_history), 'mwan': torch.Tensor(mean_r_history)}
            torch.save(reward_save, 'history/reward_history.pth')
            print('save reward history:', len(mean_r_history))
        if done:
            if (time.time()-start_time) < 30:
                pass
            else:
                break
        # env.render()
    if not isinstance(loss, int):
        pbar.set_description("loss: %s" % loss)
    pbar.update(1)
    if learn_time >= 25000 and not render:
        break

env.game_state.send_command_l('')  # release pressed keys

# plt.subplot(121)
plt.plot(iter_history, loss_history)
# plt.subplot(122)
# plt.plot(iter_history, score_history)
# plt.show()
pbar.close()
