import torch
import torch.nn.functional as F
import os
import random
from tqdm import tqdm
import matplotlib.pyplot as plt
import game_wrapped
from networks_transformer import MultiModalEncoder
from brawlstar_utils import CONFIG


class DataLoader:
    def __init__(self, path):
        self.path = path
        self.action_mix = CONFIG['action_mix']

    def load_all(self):
        Load_ = None
        for i in tuple(os.walk(self.path))[0][-1]:
            if i[-3:] == 'pth' and 'usable' not in i:
                load_ = torch.load(self.path + '/' + i)
                if Load_ is None:
                    Load_ = load_
                else:
                    for j in Load_.keys():
                        Load_[j] = torch.cat([Load_[j], load_[j]], dim=0)
        if Load_ is None:
            assert False, 'data not found'
        return Load_

    def load(self):
        return torch.load(self.path + '/' + 'data.pth')

    def get_reward(self, s4, s4_, a):
        # s4, s4_: batch * time-step * N
        r = torch.zeros((s4.shape[0], 1))  # batch * 1
        for batch in range(s4.shape[0]):
            s4_b, s4_b_, a_b = s4[batch], s4_[batch], self.action_mix[int(a[batch])]
            r_b = 0

            env.HP = []
            env.charge = []
            # s4_b_s = s4_b[0]
            for time_step_ in range(s4_b.shape[0]):
                env.HP.append(s4_b[time_step_].tolist()[1])
                env.charge.append(s4_b[time_step_].tolist()[2])
                env.last_supercharged = s4_b[time_step_].tolist()[3]
            env._history_info_process()

            s4_b_s = s4_b_[0].tolist()
            r_ = env._info_process(s4_b_s, a_b)
            r_b += r_
            for time_step_ in range(1, s4_b_.shape[0]):  # time-step
                s4_b_s = s4_b_[time_step_].tolist()
                r_ = env._info_process(s4_b_s, a_b.split('-')[0]+'-.')  # a的处理要与DQN一致
                r_b += r_
            r_b /= s4_b_.shape[0]
            r[batch] = r_b
        return r


# multi-model-encoder includes conv encoder for feature map generated by yolo-v5s,
# as well as fully connected encoder for a game-entity-info vector
c_out = CONFIG['network_c_out']  # convolution output channel
l_out = CONFIG['network_l_out']  # linear encoder output dimension
input_size = CONFIG['network_input_size']  # lstm input-size
action_dim_l = CONFIG['action_dim_l']  # left hand options
action_dim_r = CONFIG['action_dim_r']  # right hand options
action_dim_mix = CONFIG['action_dim_mix']
time_step = CONFIG['time_step']  # the net takes 'time_step' frames of data each time
device = CONFIG['device']
render = CONFIG['render']

dataloader = DataLoader('pretrain_history')
env = game_wrapped.BrawlEnv(watcher=dataloader, time_step=time_step, render=render)
env = env.unwrapped  # gym对游戏进行了封装，有些变量变得不可访问，这里使游戏内部的变量可以访问

# 两个网络，进行延迟更新，即不能每次动作都更新网络，
# 要隔一段时间进行一次更新，否则奖励累积可能爆炸
net = MultiModalEncoder(c_out, l_out, input_size, action_dim_mix, device).to(device)
net2 = MultiModalEncoder(c_out, l_out, input_size, action_dim_mix, device).to(device)

store_count = 0  # 经验池，将智能体碰到的各种情况都记录下来
# store_size = CONFIG['store_size']  # buffer size，即经验池能存多少条数据
decline = CONFIG['decline']  # 衰减系数
learn_time = CONFIG['learn_time']  # 学习次数
update_time = CONFIG['update_time']  # 隔多少个学习次数更新一次网络
gama = CONFIG['gama']  # Q值计算时的折扣因子
b_size = CONFIG['b_size']  # batch size，一次从经验池中抽取多少数据进行学习
start_study = False
loss = 0
loss_history = []
# score_history = []
iter_history = []

if os.path.exists('history/model_params.pkl'):  # and False:
    print('---------loading trained model---------')
    net.load_state_dict(torch.load('history/model_params.pkl'))
    net2.load_state_dict(torch.load('history/model_params.pkl'))
    learn_time = int(open('history/learn_time.txt', 'r').read())
    load = torch.load('history/loss_history.pth')
    iter_history = load['iter_history'].tolist()
    loss_history = load['loss_history'].tolist()
    print('learned time: ', learn_time)
    print('loss history length: ', len(loss_history))
elif not os.path.exists('history'):
    os.mkdir('history')

"""relationship: (s, s_, a, r) means s + a -> (s_, r)"""
data = dataloader.load()
store_s1, store_s2, store_s3, store_s4, store_s5 = data['s1'], data['s2'], data['s3'], data['s4'], data['s5']
store_s1_, store_s2_, store_s3_, store_s4_, store_s5_ = data['s1_'], data['s2_'], data['s3_'], data['s4_'], data['s5_']
store_action = data['a']
store_reward = dataloader.get_reward(data['info'], data['info_'], store_action)
store_size = min(store_s1.shape[0], 1000)
# store_size = 1000
print('buffer size:', store_size)


pbar = tqdm(total=8001)
for i in range(8001):
    # s = env.reset()  # 初始化游戏env，s即为state (time_step * obsv_dim)
    if learn_time % update_time == 0:
        # 每经过一个update_time，更新net2
        # 将net的权重提取、加载到net2
        # net则是实时更新的，如后面
        net2.load_state_dict(net.state_dict())

        if not isinstance(loss, int):
            loss_history.append(torch.Tensor.cpu(loss).detach())  # detach()将去除所带梯度
            # score_history.append(env.peak_score)
            iter_history.append(learn_time)

        if learn_time % (update_time * 20) == 0:
            if os.path.exists('history/model_params.pkl'):
                os.remove('history/model_params.pkl')
                print('---------权重保存数据更新---------')
            else:
                # pass
                print('---------权重数据进行保存---------')
            torch.save(net2.state_dict(), 'history/model_params.pkl')

            # 训练时有类似epsilon greedy的效果，即随着learn_time增加，也即模型愈发完善，随机action的几率会降低
            # 所以在保存模型权重的时候也要保存这个量，在加载时一并加载，否则每次重新运行初期都会大量随机动作
            with open('history/learn_time.txt', 'w') as file:
                file.write(str(learn_time))

            # loss数据保存
            save = {'iter_history': torch.Tensor(iter_history), 'loss_history': torch.Tensor(loss_history)}
            torch.save(save, 'history/loss_history.pth')

    # 随机从某处开始，连续抽取b_size(1000)条数据用于神经网络的训练
    index = random.randint(0, store_size - b_size - 1)
    b_s = [store_s1[index:index + b_size].to(device),  # each: batch * time-step * feature-map dirs
           store_s2[index:index + b_size].to(device),
           store_s3[index:index + b_size].to(device),
           store_s4[index:index + b_size].to(device),
           store_s5[index:index + b_size].to(device)]
    b_a = torch.Tensor(store_action[index:index + b_size]).long().to(device)  # b * t * 1
    b_s_ = [store_s1_[index:index + b_size].to(device),  # as b_s
            store_s2_[index:index + b_size].to(device),
            store_s3_[index:index + b_size].to(device),
            store_s4_[index:index + b_size].to(device),
            store_s5_[index:index + b_size].to(device)]
    b_r = torch.Tensor(store_reward[index:index + b_size]).to(device)  # b * t * 1

    # 抽取出的b_a为前面探索时的action记录，某些是随机选取的，某些是参考网络做出的
    # 此处的b_s有1000条，送入net生成1000对输出，每一对都是对向左向右两种操作的判断(左右各对应的q值)
    # 即[[r1_1,r1_2], [r2_1,r2_2], ...]共1000对，这一整条数据具有"两个维度"
    # gather中的1就是说，对第二个维度进行操作，b_a则是说对第二个维度每一对数据选取哪一个
    # 例如b_a是[0,1...]即第一次向左，第二次向右
    # 则gather将对应提取并整合出[r1_1,r2_2...]
    q = net(b_s).gather(1, b_a)

    # net代入b_s_的输出同样为[[r1_1,r1_2], [r2_1,r2_2], ...]，这是对下一个state做不同action对应累积reward的估计
    # 之前的话是依照历史action筛选历史reward，此处是直接根据net做出reward选择，所以此处直接选择每一对中最大的reward
    # max返回的是(max, index)元组，我们要的是值；max(1)的1与上面的gather的1类似，是对第二个维度操作

    # solution 1/2: DQN
    # q_next = net2(b_s_).detach().max(1)[0].reshape(b_size, 1)  # reshape把1*1000转置成1000*1，才能相加

    # solution 2/2: Double DQN
    q_target_next = net2(b_s_).detach()
    q_eval_next = net(b_s_).detach()
    q_next = q_target_next.gather(1, q_eval_next.argmax(axis=1).reshape(-1, 1))

    # Q value计算公式
    tq = b_r + gama * q_next

    loss = F.smooth_l1_loss(q, tq)
    # loss = net.mls(q, tq)
    net.opt.zero_grad()
    loss.backward()
    net.opt.step()
    if i % 60 == 0:
        net.dyna_lr.step()  # 更新学习率

    learn_time += 1
    if not start_study:
        print('start study')
        start_study = True
        # break

    if not isinstance(loss, int):
        pbar.set_description("loss: %s" % loss)
    pbar.update(1)
    # if learn_time >= 25000 and not render:
    #     break


# plt.subplot(121)
plt.plot(iter_history, loss_history)
# plt.subplot(122)
# plt.plot(iter_history, score_history)
plt.show()
pbar.close()
